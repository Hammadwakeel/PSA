import json
import time
import logging
import re
from datetime import datetime
from typing import Dict, Any, List, Optional

# ‚úÖ 1. Import the new getter functions
try:
    from app.core.config import get_groq_client, get_config
except ImportError:
    # Fallback for testing execution without the full app context
    import logging
    logging.getLogger(__name__).warning("Could not import config. Mocking for syntax check.")
    get_groq_client = lambda: None
    get_config = lambda: type('Config', (), {'MODEL_NAME': 'openai/gpt-oss-120b'})()

# ‚úÖ 2. Import prompt functions
from app.core.prompts import (
    get_router_agent_prompt,
    get_stream_agent_prompt,
    get_sql_agent_prompt,
    get_vector_store_agent_prompt,
    get_multi_source_agent_prompt,
    get_nosql_agent_prompt,
    get_big_data_agent_prompt,
    get_ml_agent_prompt,
    get_multi_source_judge_prompt,
    get_vector_judge_prompt,
    get_nosql_judge_prompt,
    get_sql_judge_prompt,
    get_ml_judge_prompt,
    get_general_qa_judge_prompt
)

# Setup structured logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("rivergen_agents")

# ==============================================================================
# üõ†Ô∏è HELPER: Robust JSON Parser
# ==============================================================================
def clean_and_parse_json(raw_content: str) -> Dict[str, Any]:
    """
    Production-grade JSON parser that handles common LLM formatting issues.
    """
    try:
        return json.loads(raw_content)
    except json.JSONDecodeError:
        clean_text = re.sub(r"```json\s*|\s*```", "", raw_content, flags=re.IGNORECASE).strip()
        try:
            return json.loads(clean_text)
        except json.JSONDecodeError as e:
            logger.error(f"JSON Parsing Failed. Raw content sample: {raw_content[:200]}...")
            raise ValueError(f"LLM returned invalid JSON format: {str(e)}")

# ==============================================================================
# üõ†Ô∏è HELPER: Validate Visualization Config (Text Suggestions)
# ==============================================================================
def validate_visualizations(viz_config: List[str], is_write_operation: bool = False) -> List[str]:
    """
    Validates and ensures visualization config has 2-5 text suggestions.
    Returns empty list for write operations.
    Expects simple text strings describing what to plot and graph name.
    """
    if is_write_operation:
        return []
    
    if not isinstance(viz_config, list):
        logger.warning("visualization_config is not a list, defaulting to empty")
        return []
    
    # Filter out invalid entries - should be strings
    valid_viz = []
    for viz in viz_config:
        if isinstance(viz, str) and viz.strip():
            valid_viz.append(viz.strip())
        elif isinstance(viz, dict):
            # Handle legacy format - convert to text
            graph_type = viz.get("type", "chart")
            title = viz.get("title", "Visualization")
            description = viz.get("description", "")
            text = f"{title} ({graph_type})"
            if description:
                text += f": {description}"
            valid_viz.append(text)
    
    # Ensure we have 2-5 visualizations
    if len(valid_viz) < 2:
        logger.warning(f"Only {len(valid_viz)} valid visualizations found, minimum is 2. Returning empty list.")
        return []
    elif len(valid_viz) > 5:
        logger.warning(f"Found {len(valid_viz)} visualizations, truncating to 5.")
        return valid_viz[:5]
    
    return valid_viz

# ==============================================================================
# 1. MASTER ROUTER AGENT
# ==============================================================================
def router_agent(full_payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Analyzes input to route requests.
    Includes token usage tracking for cost observability.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()
    
    start_time = time.time()
    request_id = full_payload.get("request_id", "unknown_id")
    logger.info(f"üß≠ [Router] Analyzing Request ID: {request_id}")

    # Payload Summarization
    data_sources = full_payload.get('data_sources', [])
    source_summary = []
    for ds in data_sources:
        source_summary.append({
            "name": ds.get("name"),
            "type": ds.get("type", "unknown").lower()
        })
    
    routing_context = {
        "user_prompt": full_payload.get("user_prompt"),
        "data_source_count": len(data_sources),
        "data_sources": source_summary,
        "context_roles": full_payload.get("user_context", {}).get("roles", [])
    }

    system_prompt = get_router_agent_prompt()

    try:
        completion = client.chat.completions.create(
            model=config.MODEL_NAME, # ‚úÖ Use config.MODEL_NAME
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(routing_context, indent=2)}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        raw_response = completion.choices[0].message.content
        result = clean_and_parse_json(raw_response)
        
        # ‚úÖ CAPTURE TOKENS
        # We extract usage stats directly from the completion object
        usage_stats = {
            "input_tokens": completion.usage.prompt_tokens,
            "output_tokens": completion.usage.completion_tokens,
            "total_tokens": completion.usage.total_tokens
        }
        
        # Inject usage into the result dictionary
        result["usage"] = usage_stats
        
        duration = (time.time() - start_time) * 1000
        logger.info(f"üëâ [Router] Selected: {result.get('selected_agent')} - {duration:.2f}ms")
        
        return result

    except Exception as e:
        logger.error(f"Router Agent Failed: {str(e)}", exc_info=True)
        
        # Define empty usage for fallback scenarios
        empty_usage = {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}
        
        # Fallback Logic - Try to route based on data source type
        if len(data_sources) > 1:
            return {
                "selected_agent": "multi_source_agent", 
                "confidence": 0.5, 
                "reasoning": "Fallback: Multiple sources detected.",
                "usage": empty_usage
            }
        
        # Single source fallback routing
        if data_sources:
            primary_source = data_sources[0]
            source_type = primary_source.get("type", "unknown").lower()
            
            # SQL databases
            if source_type in ['postgresql', 'mysql', 'mariadb', 'sqlserver', 'oracle']:
                return {
                    "selected_agent": "sql_agent",
                    "confidence": 0.6,
                    "reasoning": f"Fallback: SQL database type '{source_type}' detected.",
                    "usage": empty_usage
                }
            # NoSQL databases
            elif source_type in ['mongodb', 'dynamodb', 'cassandra', 'elasticsearch', 'redis']:
                return {
                    "selected_agent": "nosql_agent",
                    "confidence": 0.6,
                    "reasoning": f"Fallback: NoSQL database type '{source_type}' detected.",
                    "usage": empty_usage
                }
            # Cloud warehouses, storage, file formats, SaaS/APIs
            elif source_type in ['snowflake', 'bigquery', 'redshift', 'synapse', 'databricks',
                                's3', 'azure_blob_storage', 'gcs',
                                'csv', 'excel', 'json', 'parquet', 'orc', 'delta_lake', 'iceberg', 'hudi',
                                'salesforce', 'hubspot', 'stripe', 'jira', 'servicenow', 'rest_api', 'graphql_api']:
                return {
                    "selected_agent": "big_data_agent",
                    "confidence": 0.6,
                    "reasoning": f"Fallback: Big data/warehouse/storage type '{source_type}' detected.",
                    "usage": empty_usage
                }
            # Streaming
            elif source_type == 'kafka':
                return {
                    "selected_agent": "stream_agent",
                    "confidence": 0.6,
                    "reasoning": "Fallback: Streaming source 'kafka' detected.",
                    "usage": empty_usage
                }
            # Vector databases
            elif source_type in ['pinecone', 'weaviate']:
                return {
                    "selected_agent": "vector_store_agent",
                    "confidence": 0.6,
                    "reasoning": f"Fallback: Vector database type '{source_type}' detected.",
                    "usage": empty_usage
                }
            
        return {
            "error": "Routing Failed - Unknown data source type", 
            "selected_agent": "error_handler",
            "usage": empty_usage
        }
    
# ==============================================================================
# 2. STREAM AGENT (Hardened for Kafka/Kinesis Analytics)
# ==============================================================================
def stream_agent(payload: Dict[str, Any], feedback: str = None) -> Dict[str, Any]:
    """
    Step 3/4 (Branch D): Generates an Analytical Streaming Execution Plan.
    Hardened for Windowing, Aggregations, and Anomaly Detection.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    start_time = time.time()
    logger.info(f"üì° [Stream Agent] Generating plan... Feedback: {bool(feedback)}")

    try:
        # 1. Extract Source & Schema Context (Robust)
        data_sources = payload.get('data_sources', [])
        schema_summary = []
        known_fields = []
        
        # Default to a safe fallback ID if none found
        primary_ds_id = data_sources[0].get("data_source_id", 1) if data_sources else 1

        for ds in data_sources:
            ds_name = ds.get('name', 'Unknown Stream')
            
            # Kafka sources might use 'schemas' -> 'tables' OR specific 'topics' metadata
            # We check both to be safe.
            schemas = ds.get('schemas') or []
            topics = ds.get('topics') or []

            # Case A: Standard Schema Structure
            for schema in schemas:
                for table in schema.get('tables', []):
                    t_name = table.get('table_name')
                    cols = [c['column_name'] for c in table.get('columns', [])]
                    known_fields.extend(cols)
                    schema_summary.append(f"Source: {ds_name} | Topic: {t_name} | Fields: {', '.join(cols)}")

            # Case B: Direct Topic Definitions (Common in Kafka payloads)
            for topic in topics:
                t_name = topic.get('topic_name')
                cols = [f['field_name'] for f in topic.get('fields', [])]
                known_fields.extend(cols)
                schema_summary.append(f"Source: {ds_name} | Topic: {t_name} | Fields: {', '.join(cols)}")

        # 2. Structured Output Template
        response_template = {
            "request_id": payload.get("request_id"),
            "status": "success",
            "intent_type": "stream_analytics",
            "execution_plan": {
                "strategy": "stream_processor",
                "type": "kafka_streams_config",
                "operations": [
                    {
                        "step": 1,
                        "type": "consume_and_process",
                        "operation_type": "read_process",
                        "data_source_id": primary_ds_id,
                        "query_payload": {
                            "topic": "<<TOPIC_NAME>>",
                            "offset_strategy": "latest",
                            "windowing": {
                                "enabled": False,
                                "window_type": "tumbling", # tumbling, hopping, sliding
                                "size_seconds": 60,
                                "aggregation_functions": [] # e.g., ["avg", "sum", "count"]
                            },
                            "analytics": {
                                "calculate_moving_average": False,
                                "anomaly_detection": False,
                                "metrics": []
                            },
                            "filter_expression": {}, 
                            "limit": 1000
                        },
                        "governance_applied": {"note": "Stream encryption and PII masking applied"}
                    }
                ]
            },
            "ai_metadata": {
                "confidence_score": 0.0,
                "reasoning_steps": []
            },
            "visualization_config": [
                "Line Chart: Real-time event count over time windows",
                "Area Chart: Moving average of transaction values",
                "Bar Chart: Event distribution by category"
            ]
        }

        # 3. System Prompt
        system_prompt = get_stream_agent_prompt(
            user_prompt=payload.get('user_prompt'),
            schema_summary=schema_summary,
            known_fields=known_fields,
            response_template=response_template,
            feedback=feedback
        )

        # 4. LLM Execution
        completion = client.chat.completions.create(
            model=config.MODEL_NAME, # ‚úÖ Use config.MODEL_NAME
            messages=[
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": f"ID: {payload.get('request_id')}"}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        # 5. Parsing & Hydration
        lean_response = clean_and_parse_json(completion.choices[0].message.content)
        
        # Telemetry
        generation_time_ms = int((time.time() - start_time) * 1000)
        
        # Ensure metadata is populated even if LLM omits it
        if "ai_metadata" not in lean_response:
            lean_response["ai_metadata"] = {}
            
        lean_response["ai_metadata"]["generation_time_ms"] = generation_time_ms
        lean_response["ai_metadata"]["model"] = config.MODEL_NAME
        
        # Validate and set visualizations for stream analytics
        if "visualization_config" in lean_response:
            lean_response["visualization"] = validate_visualizations(lean_response["visualization_config"], False)
        else:
            lean_response["visualization"] = []

        return lean_response

    except Exception as e:
        logger.error(f"Stream Agent Failed: {e}", exc_info=True)
        return {"error": f"Stream Agent Failed: {str(e)}"}
    
# ==============================================================================
# 3. SQL AGENT (Relational DB Specialist)
# ==============================================================================
def sql_agent(payload: Dict[str, Any], feedback: str = None) -> Dict[str, Any]:
    """
    Step 3/4: Generates a Dialect-Aware Execution Plan.
    Enforces Transaction Safety and Literal RLS Injection.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    start_time = time.time()
    logger.info(f"üíæ [SQL Agent] Generating plan... Feedback: {bool(feedback)}")
    
    try:
        # 1. Dynamic Dialect Detection (Robust)
        data_sources = payload.get('data_sources', [])
        # Default to postgresql if no sources provided (fallback)
        primary_ds = data_sources[0] if data_sources else {}
        db_type = primary_ds.get('type', 'postgresql').lower() 
        ds_id = primary_ds.get('data_source_id', 1)
        
        # 2. Extract Context & Schema
        user_context = payload.get('user_context', {})
        user_id = user_context.get("user_id", 0)
        
        # Context variables for Injection
        context_vars = {
            "user_id": user_id,
            "org_id": user_context.get("organization_id"),
            "attributes": user_context.get("attributes", {})
        }

        schema_summary = []
        governance_instructions = []

        for ds in data_sources:
            ds_name = ds.get('name', 'Unknown Source')
            schema_name_list = []
            # Handle potentially missing 'schemas' key or None value
            schemas = ds.get('schemas') or []
            
            for schema in schemas:
                schema_name = schema.get('schema_name', 'default')
                # Handle potentially missing 'tables' key or None value
                tables = schema.get('tables') or []
                for table in tables:
                    t_name = table.get('table_name')
                    table_type = table.get('table_type', 'table')
                    # Handle potentially missing 'columns' key or None value
                    cols_data = table.get('columns') or []
                    cols = []
                    for c in cols_data:
                        col_name = c.get('column_name')
                        col_type = c.get('column_type', 'unknown')
                        is_pii = c.get('pii', False)
                        cols.append(f"{col_name} ({col_type})" + (" [PII]" if is_pii else ""))
                    
                    if cols:
                        schema_summary.append(f"Schema: {schema_name} | Table: {t_name} ({table_type}) | Columns: {', '.join(cols)}")
                        schema_name_list.append(schema_name)

            # üîí Comprehensive Governance Policy Extraction
            policies = ds.get('governance_policies', {})
            if policies:
                # Row-Level Security (RLS) Processing
                rls = policies.get("row_level_security", {})
                if rls.get("enabled"):
                    rls_rules = rls.get("rules", [])
                    for rule in rls_rules:
                        condition = rule.get("condition", "")
                        description = rule.get("description", "")
                        # Replace context variables with actual values
                        # {user_id} -> actual user_id, {user.attributes.X} -> actual attribute values
                        processed_condition = condition
                        if "{user_id}" in processed_condition:
                            processed_condition = processed_condition.replace("{user_id}", str(user_id))
                        for attr_key, attr_value in context_vars.get("attributes", {}).items():
                            attr_pattern = f"{{user.attributes.{attr_key}}}"
                            if attr_pattern in processed_condition:
                                processed_condition = processed_condition.replace(attr_pattern, f"'{attr_value}'" if isinstance(attr_value, str) else str(attr_value))
                        
                        # Determine which tables this RLS applies to based on condition
                        # Check if condition references specific tables/columns from schema
                        applicable_tables = []
                        for schema in schemas:
                            for table in schema.get('tables', []):
                                t_name = table.get('table_name')
                                # If condition mentions table name or common column patterns, apply to this table
                                if t_name.lower() in condition.lower() or any(col.get('column_name', '').lower() in condition.lower() for col in table.get('columns', [])):
                                    applicable_tables.append(t_name)
                        
                        if applicable_tables:
                            tables_str = ", ".join(applicable_tables)
                            # Check if condition references system tables - if so, emphasize literal substitution
                            if "user_access" in processed_condition.lower() or ("SELECT" in processed_condition.upper() and "user_access" in processed_condition.lower()):
                                # This is a subquery that needs to be replaced with literal
                                assigned_region = context_vars.get('attributes', {}).get('assigned_region', '')
                                governance_instructions.append(
                                    f"‚ö†Ô∏è CRITICAL RLS FOR '{ds_name}' (Tables: {tables_str}): "
                                    f"The condition '{condition}' references a system table 'user_access' which DOES NOT EXIST in the database schema. "
                                    f"You MUST replace this subquery with a LITERAL VALUE from user context. "
                                    f"Based on user context (assigned_region='{assigned_region}'), replace it with: `region = '{assigned_region}'` "
                                    f"or `region IN ('{assigned_region}')`. NEVER use subqueries to non-existent tables. "
                                    f"Description: {description}"
                                )
                            else:
                                governance_instructions.append(
                                    f"‚ö†Ô∏è MANDATORY RLS FOR '{ds_name}' (Tables: {tables_str}): "
                                    f"You MUST apply this filter in WHERE clause: {processed_condition}. "
                                    f"Description: {description}"
                                )
                        else:
                            # Apply to all tables if no specific table mentioned
                            if "user_access" in processed_condition.lower() or ("SELECT" in processed_condition.upper() and "user_access" in processed_condition.lower()):
                                assigned_region = context_vars.get('attributes', {}).get('assigned_region', '')
                                governance_instructions.append(
                                    f"‚ö†Ô∏è CRITICAL RLS FOR '{ds_name}' (ALL TABLES): "
                                    f"The condition '{condition}' references a system table 'user_access' which DOES NOT EXIST. "
                                    f"You MUST replace this subquery with a LITERAL VALUE: `region = '{assigned_region}'` or `region IN ('{assigned_region}')`. "
                                    f"NEVER use subqueries to non-existent tables. Description: {description}"
                                )
                            else:
                                governance_instructions.append(
                                    f"‚ö†Ô∏è MANDATORY RLS FOR '{ds_name}' (ALL TABLES): "
                                    f"You MUST apply this filter in WHERE clause: {processed_condition}. "
                                    f"Description: {description}"
                                )
                
                # Column Masking Processing
                masking = policies.get("column_masking", {})
                if masking.get("enabled"):
                    masking_rules = masking.get("rules", [])
                    for rule in masking_rules:
                        if isinstance(rule, dict):
                            column = rule.get("column", "")
                            condition = rule.get("condition", "")
                            masking_function = rule.get("masking_function", "mask")
                            description = rule.get("description", "")
                            
                            # Process condition with context variables
                            processed_condition = condition
                            if "{user_id}" in processed_condition:
                                processed_condition = processed_condition.replace("{user_id}", str(user_id))
                            for attr_key, attr_value in context_vars.get("attributes", {}).items():
                                attr_pattern = f"{{user.attributes.{attr_key}}}"
                                if attr_pattern in processed_condition:
                                    processed_condition = processed_condition.replace(attr_pattern, f"'{attr_value}'" if isinstance(attr_value, str) else str(attr_value))
                            
                            # Generate masking SQL based on masking function and database type
                            if masking_function == "email_mask":
                                if db_type.lower() == "postgresql":
                                    mask_sql = f"CASE WHEN {processed_condition} THEN {column} ELSE CONCAT(LEFT({column}, 3), '***@', SPLIT_PART({column}, '@', 2)) END"
                                elif db_type.lower() in ["mysql", "mariadb"]:
                                    mask_sql = f"CASE WHEN {processed_condition} THEN {column} ELSE CONCAT(LEFT({column}, 3), '***@', SUBSTRING_INDEX({column}, '@', -1)) END"
                                elif db_type.lower() == "sqlserver":
                                    mask_sql = f"CASE WHEN {processed_condition} THEN {column} ELSE CONCAT(LEFT({column}, 3), '***@', RIGHT({column}, CHARINDEX('@', REVERSE({column})) - 1)) END"
                                elif db_type.lower() == "oracle":
                                    mask_sql = f"CASE WHEN {processed_condition} THEN {column} ELSE SUBSTR({column}, 1, 3) || '***@' || SUBSTR({column}, INSTR({column}, '@') + 1) END"
                                else:
                                    mask_sql = f"CASE WHEN {processed_condition} THEN {column} ELSE '***MASKED***' END"
                            elif masking_function == "mask":
                                mask_sql = f"CASE WHEN {processed_condition} THEN {column} ELSE '***MASKED***' END"
                            else:
                                mask_sql = f"CASE WHEN {processed_condition} THEN {column} ELSE '***MASKED***' END"
                            
                            # Add specific guidance for email masking
                            if masking_function == "email_mask":
                                governance_instructions.append(
                                    f"‚ö†Ô∏è MANDATORY COLUMN MASKING FOR '{ds_name}' (Column: {column}): "
                                    f"In SELECT clause, use proper email masking format that preserves domain part. "
                                    f"DO NOT use fixed-length SUBSTR like SUBSTR(email, 15) - emails have variable lengths. "
                                    f"Use this format for {db_type.upper()}: {mask_sql} AS {column}. "
                                    f"The masking condition '{processed_condition}' determines when to mask (if condition is false, mask the email). "
                                    f"Description: {description}"
                                )
                            else:
                                governance_instructions.append(
                                    f"‚ö†Ô∏è MANDATORY COLUMN MASKING FOR '{ds_name}' (Column: {column}): "
                                    f"In SELECT clause, use: {mask_sql} AS {column}. "
                                    f"Description: {description}"
                                )
                        elif isinstance(rule, str):
                            # Simple column name rule - for email, provide proper masking instruction
                            if rule.lower() == "email":
                                if db_type.lower() == "postgresql":
                                    mask_example = "CONCAT(LEFT(email, 3), '***@', SUBSTRING(email, POSITION('@' IN email) + 1))"
                                elif db_type.lower() in ["mysql", "mariadb"]:
                                    mask_example = "CONCAT(LEFT(email, 3), '***@', SUBSTRING_INDEX(email, '@', -1))"
                                else:
                                    mask_example = "CONCAT(LEFT(email, 3), '***@', SUBSTRING(email, CHARINDEX('@', email) + 1))"
                                governance_instructions.append(
                                    f"‚ö†Ô∏è MANDATORY COLUMN MASKING FOR '{ds_name}' (Column: {rule}): "
                                    f"Mask email column using proper format. DO NOT use fixed-length SUBSTR. "
                                    f"Use format like: {mask_example} AS email. Preserve domain part after @ symbol."
                                )
                            else:
                                governance_instructions.append(
                                    f"‚ö†Ô∏è MANDATORY COLUMN MASKING FOR '{ds_name}' (Column: {rule}): "
                                    f"Mask this column in SELECT clause using appropriate masking function."
                                )

        # 3. Lean Template
        lean_template = {
            "intent_summary": "<<BRIEF_SUMMARY>>",
            "sql_statement": f"<<VALID_{db_type.upper()}_SQL>>",
            "governance_explanation": "<<CONFIRM_RLS>>",
            "confidence_score": 0.0,
            "reasoning_steps": ["<<STEP_1>>", "<<STEP_2>>"],
            "visualization_config": [
                "Bar Chart: Total sales by product category",
                "Line Graph: Monthly revenue trend over the past year"
            ],
            "suggestions": []
        }

        # 4. System Prompt (Dialect-Aware)
        system_prompt = get_sql_agent_prompt(
            db_type=db_type,
            governance_instructions=governance_instructions,
            schema_summary=schema_summary,
            lean_template=lean_template,
            feedback=feedback
        )

        # 5. Execute LLM Call
        completion = client.chat.completions.create(
            model=config.MODEL_NAME, # ‚úÖ Use config.MODEL_NAME
            messages=[
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": f"Request ID: {payload.get('request_id')}"}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        # 6. Parse LLM Response (Safe Parsing)
        lean_response = clean_and_parse_json(completion.choices[0].message.content)
        
        # Telemetry
        end_time = time.time()
        generation_time_ms = int((end_time - start_time) * 1000)
        
        # Determine operation type based on SQL keyword
        sql_stmt = lean_response.get("sql_statement", "")
        op_type = "read" if "SELECT" in sql_stmt.upper() and "INSERT" not in sql_stmt.upper() else "write"

        # 7. Hydrate Full Response
        final_plan = {
            "request_id": payload.get("request_id"),
            "execution_id": payload.get("execution_id"),
            "plan_id": f"plan-{payload.get('request_id')}",
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "intent_type": "analytical_query",
            "intent_summary": lean_response.get("intent_summary", ""),
            "execution_plan": {
                "strategy": "pushdown",
                "type": "sql_query",
                "operations": [{
                    "step": 1,
                    "operation_type": op_type,
                    "compute_engine": db_type, # Dynamic based on source
                    "query": sql_stmt,
                    "query_payload": {
                        "language": "sql",
                        "dialect": db_type, # Dynamic based on source
                        "statement": sql_stmt
                    },
                    "governance_applied": {"rls_rules": governance_instructions}
                }]
            },
            "visualization": validate_visualizations(lean_response.get("visualization_config", []), op_type == "write"),
            "ai_metadata": {
                "generation_time_ms": generation_time_ms,
                "confidence_score": lean_response.get("confidence_score", 0.0),
                "explanation": lean_response.get("governance_explanation", ""),
                "reasoning_steps": lean_response.get("reasoning_steps", [])
            },
            "suggestions": lean_response.get("suggestions", [])
        }
        return final_plan

    except Exception as e:
        logger.error(f"SQL Agent Failed: {e}", exc_info=True)
        return {"error": f"SQL Agent Failed: {str(e)}"}
        
# ==============================================================================
# 4. VECTOR STORE AGENT (Similarity & Rejection Logic)
# ==============================================================================
def vector_store_agent(payload: Dict[str, Any], feedback: str = None) -> Dict[str, Any]:
    """
    Step 3/4: Generates a RiverGen Execution Plan for Vector Databases.
    Hardened for strict Judge compliance and correct query payload structure.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    start_time = time.time()
    logger.info(f"üéØ [Vector Agent] Generating plan... Feedback: {bool(feedback)}")

    try:
        # 1. Extract Context & Schema (Robust)
        data_sources = payload.get("data_sources", [])
        primary_ds = data_sources[0] if data_sources else {}
        ds_id = primary_ds.get("data_source_id")
        ds_name = primary_ds.get("name")
        db_type = primary_ds.get("type", "vector")
        
        # Execution Context
        exec_ctx = payload.get("execution_context", {})
        default_top_k = exec_ctx.get("max_rows", 10)

        # Schema Analysis
        schema_summary = []
        valid_metadata_fields = []
        
        # Handle cases where 'schemas' is None or empty
        schemas = primary_ds.get("schemas") or []
        
        for schema in schemas:
            for table in schema.get("tables", []) or []:
                t_name = table.get('table_name')
                cols_data = table.get('columns') or []
                cols = []
                
                for c in cols_data:
                    col_name = c.get('column_name')
                    col_type = c.get('column_type', 'unknown')
                    cols.append(f"{col_name} ({col_type})")
                    
                    # Identify valid metadata fields for filtering
                    # Exclude actual vector blobs and IDs from being filter targets
                    if "vector" not in col_type.lower() and col_name != "id":
                        valid_metadata_fields.append(col_name)
                
                schema_summary.append(f"Index: {t_name} | Fields: {', '.join(cols)}")

        # 2. Lean Template
        lean_template = {
            "intent_summary": "<<BRIEF_SUMMARY>>",
            "vector_search_config": {
                "index_name": "<<INDEX_NAME_FROM_SCHEMA>>", 
                "vector_column": "<<VECTOR_COLUMN_FROM_SCHEMA>>",
                "query_text": "<<SEMANTIC_SEARCH_TEXT>>", # e.g. "wireless headphones"
                "top_k": 10,
                "filters": {} # e.g. {"product_id": "123"}
            },
            "reasoning_steps": ["<<STEP_1>>", "<<STEP_2>>"],
            "suggestions": ["<<SUGGESTION>>"]
        }

        # 3. System Prompt
        system_prompt = get_vector_store_agent_prompt(
            user_prompt=payload.get('user_prompt'),
            db_type=db_type,
            default_top_k=default_top_k,
            schema_summary=schema_summary,
            valid_metadata_fields=valid_metadata_fields,
            lean_template=lean_template,
            feedback=feedback
        )

        # 4. LLM Generation
        completion = client.chat.completions.create(
            model=config.MODEL_NAME, # ‚úÖ Use config.MODEL_NAME
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Request ID: {payload.get('request_id')}"}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        # Telemetry
        end_time = time.time()
        generation_time_ms = int((end_time - start_time) * 1000)
        input_tokens = completion.usage.prompt_tokens
        output_tokens = completion.usage.completion_tokens
        
        # Parse Response
        lean_response = clean_and_parse_json(completion.choices[0].message.content)
        vs_config = lean_response.get("vector_search_config", {})

        # 5. Construct Final Payload (The "Format" You Requested)
        query_text = vs_config.get("query_text", payload.get('user_prompt'))
        
        final_plan = {
            "request_id": payload.get("request_id"),
            "execution_id": payload.get("execution_id", f"exec-{payload.get('request_id')}"),
            "plan_id": f"plan-{int(time.time())}",
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "intent_type": "query",
            "intent_summary": lean_response.get("intent_summary", "Vector Search"),
            "execution_plan": {
                "strategy": "pushdown",
                "type": "vector_search",
                "operations": [
                    {
                        "step": 1,
                        "step_id": "op-1",
                        "operation_type": "read",
                        "type": "source_query",
                        "description": lean_response.get("intent_summary"),
                        "data_source_id": ds_id,
                        "compute_type": "source_native",
                        "compute_engine": db_type,
                        "dependencies": [],
                        "query": f"search('{query_text}', k={vs_config.get('top_k', 10)})",
                        "query_payload": {
                            "language": "vector",
                            "dialect": None,
                            "statement": f"search('{query_text}')",
                            # THIS IS THE CRITICAL PART FOR THE JUDGE:
                            "parameters": {
                                "index_name": vs_config.get("index_name"),
                                "vector_column": vs_config.get("vector_column"),
                                "query_vector_text": query_text, 
                                "top_k": vs_config.get("top_k", 10),
                                "filters": vs_config.get("filters", {}),
                                "search_params": {
                                    "metric": "cosine",
                                    "queries": [query_text] # Non-empty array required by Judge
                                }
                            }
                        },
                        "governance_applied": {
                            "rls_rules": [],
                            "masking_rules": []
                        },
                        "output_artifact": "similar_vectors"
                    }
                ]
            },
            "visualization": None,
            "ai_metadata": {
                "model": config.MODEL_NAME,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "generation_time_ms": generation_time_ms,
                "confidence": 0.95, # High confidence because we force-filled the query
                "confidence_score": 0.95,
                "explanation": "Performed vector similarity search using the provided schema.",
                "reasoning_steps": lean_response.get("reasoning_steps", [])
            },
            "suggestions": lean_response.get("suggestions", [])
        }

        return final_plan

    except Exception as e:
        logger.error(f"Vector Agent Failed: {e}", exc_info=True)
        return {"error": f"Vector Agent Failed: {str(e)}"}
    
# ==============================================================================
# 5. MULTI-SOURCE AGENT (Federated Trino/ANSI SQL)
# ==============================================================================
def multi_source_agent(payload: Dict[str, Any], feedback: str = None) -> Dict[str, Any]:
    """
    Step 3/4 (Branch B): Generates a Hybrid/Federated Execution Plan.
    Hardened for System Table Injection and Multi-Hop Joins.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    start_time = time.time()
    logger.info(f"üåê [Multi-Source Agent] Generating hybrid plan... Feedback: {bool(feedback)}")

    try:
        # 1. Extract Context & Schema (Robust)
        data_sources = payload.get('data_sources', [])
        user_context = payload.get('user_context', {})
        user_id = user_context.get("user_id", 0)
        
        context_vars = {
            "user_id": user_id,
            "org_id": user_context.get("organization_id"),
            "attributes": user_context.get("attributes", {})
        }

        schema_summary = []
        governance_instructions = []
        source_map = {} 

        for ds in data_sources:
            ds_id = ds.get('data_source_id')
            ds_name = ds.get('name')
            ds_type = ds.get('type')
            source_map[ds_name] = ds_id 
            
            # Robust Schema Extraction (Null-Safe)
            schemas_list = ds.get('schemas') or []
            file_meta = ds.get('file_metadata') or {}
            file_schemas = file_meta.get('schemas') or []
            curr_schemas = schemas_list + file_schemas

            for schema in curr_schemas:
                for table in schema.get('tables', []) or []:
                    t_name = table.get('table_name')
                    cols_data = table.get('columns') or []
                    cols = [c.get('column_name') for c in cols_data if c.get('column_name')]
                    
                    if cols:
                        schema_summary.append(f"SOURCE (ID {ds_id}) [{ds_type}] '{ds_name}' -> Table: {t_name} | Columns: {', '.join(cols)}")

            # Governance - Enhanced for Multi-Source
            policies = ds.get('governance_policies', {})
            if policies:
                # Row-Level Security (RLS) Processing
                rls = policies.get("row_level_security", {})
                if rls.get("enabled"):
                    rls_rules = rls.get("rules", [])
                    for rule in rls_rules:
                        condition = rule.get("condition", "")
                        description = rule.get("description", "")
                        # Replace context variables with actual values
                        processed_condition = condition
                        if "{user_id}" in processed_condition:
                            processed_condition = processed_condition.replace("{user_id}", str(user_id))
                        for attr_key, attr_value in context_vars.get("attributes", {}).items():
                            attr_pattern = f"{{user.attributes.{attr_key}}}"
                            if attr_pattern in processed_condition:
                                processed_condition = processed_condition.replace(attr_pattern, f"'{attr_value}'" if isinstance(attr_value, str) else str(attr_value))
                        
                        # For multi-source, use VALUES clause or literal substitution if system table referenced
                        if "user_access" in processed_condition.lower() or "SELECT" in processed_condition.upper():
                            # Replace subquery with literal values from context
                            governance_instructions.append(
                                f"‚ö†Ô∏è MANDATORY RLS FOR '{ds_name}' (Source ID {ds_id}): "
                                f"Apply filter: {processed_condition}. "
                                f"If condition references system tables like 'user_access', replace with literal values from context: {json.dumps(context_vars)}. "
                                f"Use fully qualified table names (e.g., {ds_name}.schema.table) when applying RLS. "
                                f"Description: {description}"
                            )
                        else:
                            governance_instructions.append(
                                f"‚ö†Ô∏è MANDATORY RLS FOR '{ds_name}' (Source ID {ds_id}): "
                                f"You MUST apply this filter in WHERE clause: {processed_condition}. "
                                f"Use fully qualified table names (e.g., {ds_name}.schema.table). "
                                f"Description: {description}"
                            )
                
                # Column Masking Processing
                masking = policies.get("column_masking", {})
                if masking.get("enabled"):
                    masking_rules = masking.get("rules", [])
                    for rule in masking_rules:
                        if isinstance(rule, dict):
                            column = rule.get("column", "")
                            condition = rule.get("condition", "")
                            masking_function = rule.get("masking_function", "mask")
                            description = rule.get("description", "")
                            
                            # Process condition with context variables
                            processed_condition = condition
                            if "{user_id}" in processed_condition:
                                processed_condition = processed_condition.replace("{user_id}", str(user_id))
                            for attr_key, attr_value in context_vars.get("attributes", {}).items():
                                attr_pattern = f"{{user.attributes.{attr_key}}}"
                                if attr_pattern in processed_condition:
                                    processed_condition = processed_condition.replace(attr_pattern, f"'{attr_value}'" if isinstance(attr_value, str) else str(attr_value))
                            
                            # Generate masking SQL (use standard SQL that works across databases)
                            if masking_function == "email_mask":
                                mask_sql = f"CASE WHEN {processed_condition} THEN {ds_name}.{column} ELSE CONCAT(LEFT({ds_name}.{column}, 3), '***@', SUBSTRING({ds_name}.{column}, CHARINDEX('@', {ds_name}.{column}) + 1)) END"
                            else:
                                mask_sql = f"CASE WHEN {processed_condition} THEN {ds_name}.{column} ELSE '***MASKED***' END"
                            
                            governance_instructions.append(
                                f"‚ö†Ô∏è MANDATORY COLUMN MASKING FOR '{ds_name}' (Column: {column}): "
                                f"In SELECT clause, use: {mask_sql} AS {column}. "
                                f"Description: {description}"
                            )
                        elif isinstance(rule, str):
                            governance_instructions.append(
                                f"‚ö†Ô∏è MANDATORY COLUMN MASKING FOR '{ds_name}' (Column: {rule}): "
                                f"Mask this column in SELECT clause using appropriate masking function."
                            )

        # 2. Lean Template (Force 'trino_sql' type for correct Judging)
        lean_template = {
            "intent_summary": "<<BRIEF_SUMMARY>>",
            "intent_type": "join", 
            "confidence_score": 0.0, 
            "execution_plan": {
                "strategy": "hybrid",
                "type": "trino_sql",  # Forces Multi-Source Judge
                "operations": [
                    {
                        "step": 1,
                        "step_id": "<<UNIQUE_ID>>",
                        "operation_type": "read", 
                        "type": "source_query",
                        "description": "<<DESC>>",
                        "data_source_id": 1, 
                        "compute_type": "source_native", 
                        "compute_engine": "<<ENGINE>>", 
                        "dependencies": [],
                        "query": "<<SQL_QUERY>>",
                        "query_payload": {
                            "language": "sql",
                            "dialect": "<<DIALECT>>", 
                            "statement": "<<SQL_QUERY>>",
                            "parameters": []
                        },
                        "governance_applied": {
                            "rls_rules": ["<<RULE_NAME>>"],
                            "masking_rules": []
                        },
                        "output_artifact": "<<ARTIFACT_NAME>>"
                    }
                ]
            },
            "reasoning_steps": ["<<STEP_1>>", "<<STEP_2>>"],
            "suggestions": ["<<SUGGESTION>>"],
            "visualization_config": [
                "Bar Chart: Total revenue by customer segment across sources",
                "Line Graph: Sales trend comparison between regions"
            ]
        }

        # 3. System Prompt
        system_prompt = get_multi_source_agent_prompt(
            schema_summary=schema_summary,
            governance_instructions=governance_instructions,
            context_vars=context_vars,
            lean_template=lean_template,
            feedback=feedback
        )

        # 4. LLM Call & Hydration
        completion = client.chat.completions.create(
            model=config.MODEL_NAME, # ‚úÖ Use config.MODEL_NAME
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Request ID: {payload.get('request_id')}"}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        # Telemetry
        end_time = time.time()
        generation_time_ms = int((end_time - start_time) * 1000)
        input_tokens = completion.usage.prompt_tokens
        output_tokens = completion.usage.completion_tokens
        
        # Parse Response using Helper
        lean_response = clean_and_parse_json(completion.choices[0].message.content)
        
        # Dynamic Values
        ai_confidence = lean_response.get("confidence_score", 0.0)
        viz_config = validate_visualizations(lean_response.get("visualization_config", []), False)

        final_plan = {
            "request_id": payload.get("request_id"),
            "execution_id": payload.get("execution_id", f"exec-{payload.get('request_id')}"),
            "plan_id": f"plan-{int(time.time())}", 
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "intent_type": lean_response.get("intent_type", "join"),
            "intent_summary": lean_response.get("intent_summary", ""),
            "execution_plan": lean_response.get("execution_plan", {}), 
            "visualization": viz_config, 
            "ai_metadata": {
                "model": config.MODEL_NAME,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "generation_time_ms": generation_time_ms,
                "confidence": ai_confidence,      
                "confidence_score": ai_confidence, 
                "explanation": lean_response.get("intent_summary"),
                "reasoning_steps": lean_response.get("reasoning_steps", [])
            },
            "suggestions": lean_response.get("suggestions", [])
        }

        return final_plan

    except Exception as e:
        logger.error(f"Multi-Source Agent Failed: {e}", exc_info=True)
        return {"error": f"Multi-Source Agent Failed: {str(e)}"}
    
# ==============================================================================
# 6. LLM JUDGE (The Quality Gate)
# ==============================================================================
def llm_judge(original_payload: Dict[str, Any], generated_plan: Dict[str, Any]) -> Dict[str, Any]:
    """
    Step 5: Universal Quality Gate.
    Dynamically applies specialized validation rules for SQL, NoSQL, Vector, Stream, ML, or Generic plans.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    try:
        # 1. Identify Plan Type
        execution_plan = generated_plan.get("execution_plan", {})
        plan_type = execution_plan.get("type", "unknown").lower()

        # 2. Parse Valid Schema Context
        data_sources = original_payload.get("data_sources", [])
        valid_schema_context = []

        for ds in data_sources:
            ds_name = ds.get("name")
            ds_id = ds.get("data_source_id")

            # üõ°Ô∏è ROBUST PARSING FOR JUDGE
            # Handle None explicitly using 'or []'
            schemas = ds.get("schemas") or [] 
            
            # If standard schemas are empty/null, check file_metadata
            if not schemas:
                 file_meta = ds.get("file_metadata") or {}
                 schemas = file_meta.get("schemas") or []

            for schema in schemas:
                tables = schema.get("tables") or []
                for table in tables:
                    valid_schema_context.append({
                        "data_source_id": ds_id,
                        "source": ds_name,
                        "object": table.get("table_name"),
                        "columns": [c['column_name'].lower() for c in (table.get('columns') or [])]
                    })
            
            # Kafka topics
            topics = ds.get("topics") or []
            for topic in topics:
                valid_schema_context.append({
                    "data_source_id": ds_id,
                    "source": ds_name,
                    "object": topic.get("topic_name"),
                    "columns": [f['field_name'].lower() for f in (topic.get('fields') or [])]
                })

        # üõ°Ô∏è System Whitelist
        valid_schema_context.append({
            "source": "SYSTEM_SECURITY",
            "object": "user_access",
            "columns": ["user_id", "region", "role", "permissions", "organization_id"]
        })
        
        # 3. Specialized Prompts
        user_prompt = original_payload.get("user_prompt")
        
        multi_source_judge_prompt = get_multi_source_judge_prompt(
            user_prompt=user_prompt,
            valid_schema_context=valid_schema_context,
            generated_plan=generated_plan
        )

        vector_judge_prompt = get_vector_judge_prompt(
            user_prompt=user_prompt,
            valid_schema_context=valid_schema_context,
            generated_plan=generated_plan
        )

        nosql_judge_prompt = get_nosql_judge_prompt(
            user_prompt=user_prompt,
            valid_schema_context=valid_schema_context,
            generated_plan=generated_plan
        )

        sql_judge_prompt = get_sql_judge_prompt(
            user_prompt=user_prompt,
            valid_schema_context=valid_schema_context,
            generated_plan=generated_plan,
            compute_engine=generated_plan.get('execution_plan', {}).get('operations', [{}])[0].get('compute_engine', 'unknown')
        )

        ml_judge_prompt = get_ml_judge_prompt(
            user_prompt=user_prompt,
            valid_schema_context=valid_schema_context,
            generated_plan=generated_plan
        )

        general_qa_judge_prompt = get_general_qa_judge_prompt(
            user_prompt=user_prompt,
            valid_schema_context=valid_schema_context,
            generated_plan=generated_plan
        )

        # 4. Select the proper prompt
        if plan_type == "vector_search":
            logger.info("üß† Using Vector Store Judge Prompt")
            system_prompt = vector_judge_prompt
        elif plan_type == "nosql_query":
            logger.info("üß† Using NoSQL Judge Prompt")
            system_prompt = nosql_judge_prompt
        elif plan_type == "trino_sql":
            logger.info("üß† Using Multi-Source Judge Prompt")
            system_prompt = multi_source_judge_prompt
        elif plan_type == "sql_query":
            logger.info("üß† Using SQL Judge Prompt")
            system_prompt = sql_judge_prompt
        elif plan_type == "ml_workflow":
            logger.info("üß† Using ML Judge Prompt")
            system_prompt = ml_judge_prompt
        else:
            logger.info("üß† Using General QA Judge Prompt")
            system_prompt = general_qa_judge_prompt

        # 5. Call LLM
        completion = client.chat.completions.create(
            model=config.MODEL_NAME,
            messages=[{"role": "system", "content": system_prompt}],
            temperature=0,
            response_format={"type": "json_object"}
        )

        # 1. Parse content first
        result = clean_and_parse_json(completion.choices[0].message.content)

        # 2. Add usage stats (Safe now because result is a dict)
        result["usage"] = {
            "input_tokens": completion.usage.prompt_tokens,
            "output_tokens": completion.usage.completion_tokens,
            "total_tokens": completion.usage.total_tokens
        }

        # 3. Return the complete object
        return result

    except Exception as e:
        logger.error(f"Judge Logic Error: {e}", exc_info=True)
        # Ensure fallback return structure matches the success structure
        return {
            "approved": False, 
            "feedback": f"Judge Logic Error: {str(e)}",
            "usage": {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}
        }
# ==============================================================================
# 7. NOSQL AGENT (NoSQL/Document DB Specialist)
# ==============================================================================
def nosql_agent(payload: Dict[str, Any], feedback: str = None) -> Dict[str, Any]:
    """
    Step 3/4: Generates a RiverGen Execution Plan for NoSQL Databases.
    Supported: MongoDB, Redis, Cassandra, DynamoDB.
    Hardened for Strict Schema Enforcement and Token Optimization.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    start_time = time.time()
    logger.info(f"üì¶ [NoSQL Agent] Generating optimized plan... Feedback: {bool(feedback)}")

    try:
        # 1. Extract Context & Schema (Robust)
        data_sources = payload.get("data_sources", [])
        primary_ds = data_sources[0] if data_sources else {}
        ds_id = primary_ds.get("data_source_id")
        ds_name = primary_ds.get("name")
        db_type = primary_ds.get("type", "generic_nosql").lower()
        
        # Execution Context
        exec_ctx = payload.get("execution_context", {})
        max_rows = exec_ctx.get("max_rows", 1000)

        # Schema Extraction
        schema_summary = []
        known_fields = set()
        
        # Handle cases where 'schemas' is None
        schemas = primary_ds.get("schemas") or []
        
        for schema in schemas:
            for table in schema.get("tables", []) or []:
                fields = []
                cols_data = table.get("columns") or []
                
                for col in cols_data:
                    c_name = col.get('column_name')
                    c_type = col.get('column_type', 'unknown')
                    if c_name:
                        fields.append(f"{c_name} ({c_type})")
                        known_fields.add(c_name.lower())
                        
                schema_summary.append(
                    f"Collection/Key: {table.get('table_name')} | Fields: {', '.join(fields)}"
                )

        # Governance Context - Enhanced for NoSQL
        governance_instructions = []
        user_context = payload.get('user_context', {})
        user_id = user_context.get("user_id", 0)
        context_vars = {
            "user_id": user_id,
            "org_id": user_context.get("organization_id"),
            "attributes": user_context.get("attributes", {})
        }
        
        policies = primary_ds.get("governance_policies", {})
        if policies:
            # Row-Level Security (RLS) for NoSQL
            rls = policies.get("row_level_security", {})
            if rls.get("enabled"):
                rls_rules = rls.get("rules", [])
                for rule in rls_rules:
                    condition = rule.get("condition", "")
                    description = rule.get("description", "")
                    # Process condition with context variables
                    processed_condition = condition
                    if "{user_id}" in processed_condition:
                        processed_condition = processed_condition.replace("{user_id}", str(user_id))
                    for attr_key, attr_value in context_vars.get("attributes", {}).items():
                        attr_pattern = f"{{user.attributes.{attr_key}}}"
                        if attr_pattern in processed_condition:
                            processed_condition = processed_condition.replace(attr_pattern, f"'{attr_value}'" if isinstance(attr_value, str) else str(attr_value))
                    
                    governance_instructions.append(
                        f"‚ö†Ô∏è MANDATORY RLS FOR '{ds_name}' ({db_type.upper()}): "
                        f"You MUST apply this filter to your query: {processed_condition}. "
                        f"Description: {description}. "
                        f"Apply using appropriate {db_type.upper()} query syntax (e.g., $match for MongoDB, FilterExpression for DynamoDB, WHERE for Cassandra, query DSL for Elasticsearch)."
                    )
            
            # Column Masking
            masking = policies.get("column_masking", {})
            if masking.get("enabled"):
                masking_rules = masking.get("rules", [])
                masked_fields = []
                for rule in masking_rules:
                    if isinstance(rule, dict):
                        masked_fields.append(rule.get("column", ""))
                    elif isinstance(rule, str):
                        masked_fields.append(rule)
                
                if masked_fields:
                    governance_instructions.append(
                        f"‚ö†Ô∏è MANDATORY COLUMN MASKING FOR '{ds_name}': "
                        f"You must exclude or mask these fields in your query: {', '.join(masked_fields)}. "
                        f"For MongoDB: use $project to exclude. For DynamoDB: exclude from ProjectionExpression. "
                        f"For Elasticsearch: exclude from _source. For Cassandra: do not SELECT these columns."
                    )
                else:
                    governance_instructions.append(
                        f"‚ö†Ô∏è MASKING REQUIRED: You must exclude or mask fields as specified in schema PII markers."
                    )

        # 2. Define "Lean" Template
        lean_template = {
            "intent_summary": "<<BRIEF_SUMMARY>>",
            "nosql_statement": "<<VALID_QUERY_STRING>>",
            "validation": {
                "schema_matches": True,
                "missing_fields": ["<<FIELD_NOT_IN_SCHEMA>>"],
                "notes": ["<<EXPLAIN_OMISSIONS>>"]
            },
            "governance_applied": {
                "rls_rules": [],
                "masking_rules": ["<<APPLIED_MASKING>>"]
            },
            "confidence_score": 0.0,
            "reasoning_steps": ["<<STEP_1>>", "<<STEP_2>>"],
            "suggestions": ["<<Q1>>"],
            "visualization_config": [
                "Bar Chart: Description of what to plot",
                "Line Graph: Another visualization suggestion"
            ]
        }

        system_prompt = get_nosql_agent_prompt(
            user_prompt=payload.get('user_prompt'),
            db_type=db_type,
            max_rows=max_rows,
            schema_summary=schema_summary,
            governance_instructions=governance_instructions,
            lean_template=lean_template,
            feedback=feedback
        )

        # 4. LLM Call & Telemetry
        completion = client.chat.completions.create(
            model=config.MODEL_NAME, # ‚úÖ Use config.MODEL_NAME
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Request ID: {payload.get('request_id')}"}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        end_time = time.time()
        generation_time_ms = int((end_time - start_time) * 1000)
        
        # Telemetry
        input_tokens = completion.usage.prompt_tokens
        output_tokens = completion.usage.completion_tokens
        
        # Parse Lean Response
        lean_response = clean_and_parse_json(completion.choices[0].message.content)

        # 5. Hydrate Full Response
        final_plan = {
            "request_id": payload.get("request_id"),
            "execution_id": payload.get("execution_id", f"exec-{payload.get('request_id')}"),
            "plan_id": f"plan-{int(time.time())}",
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "intent_type": "query" if not lean_response.get("validation", {}).get("missing_fields") else "partial_query",
            "intent_summary": lean_response.get("intent_summary", "NoSQL Query Execution"),
            "execution_plan": {
                "strategy": "pushdown",
                "type": "nosql_query",
                "operations": [
                    {
                        "step": 1,
                        "step_id": "op-1",
                        "operation_type": "read",
                        "type": "source_query",
                        "description": lean_response.get("intent_summary"),
                        "data_source_id": ds_id,
                        "compute_type": "source_native",
                        "compute_engine": db_type,
                        "dependencies": [],
                        "query": lean_response.get("nosql_statement"),
                        "query_payload": {
                            "language": db_type,
                            "dialect": None,
                            "statement": lean_response.get("nosql_statement"),
                            "parameters": []
                        },
                        "governance_applied": lean_response.get("governance_applied", {}),
                        "output_artifact": "result_cursor"
                    }
                ]
            },
            "visualization": validate_visualizations(lean_response.get("visualization_config", []), False),
            "ai_metadata": {
                "model": config.MODEL_NAME,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "generation_time_ms": generation_time_ms,
                "confidence": lean_response.get("confidence_score", 0.0),
                "confidence_score": lean_response.get("confidence_score", 0.0),
                "explanation": lean_response.get("validation", {}).get("notes", ["Execution successful"])[0],
                "reasoning_steps": lean_response.get("reasoning_steps", [])
            },
            "suggestions": lean_response.get("suggestions", [])
        }

        # Add validation warnings to the top level if needed
        if lean_response.get("validation", {}).get("missing_fields"):
            final_plan["warnings"] = [
                f"Missing fields: {', '.join(lean_response['validation']['missing_fields'])}"
            ]

        return final_plan

    except Exception as e:
        logger.error(f"NoSQL Agent Failed: {e}", exc_info=True)
        return {"error": f"NoSQL Agent Failed: {str(e)}"}
    
# ==============================================================================
# 8. BIG DATA AGENT (Hadoop/Spark Specialist)
# ==============================================================================
def big_data_agent(payload: Dict[str, Any], feedback: str = None) -> Dict[str, Any]:
    """
    Step 3/4: Generates a RiverGen Execution Plan for Big Data workloads.
    Handles Cloud Warehouses (Snowflake, BigQuery) and Data Lakes (S3, Parquet).
    Supports Self-Correction Loop via 'feedback'.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    start_time = time.time()
    logger.info(f"üêò [Big Data Agent] Generating plan... Feedback: {bool(feedback)}")

    try:
        # 1. Extract Governance & Schema Context (Robust)
        data_sources = payload.get('data_sources', [])
        governance_context = []
        source_type_hint = "unknown"
        
        # Default ID for template
        primary_ds_id = data_sources[0].get("data_source_id") if data_sources else None

        for ds in data_sources:
            # Capture the specific type to guide the prompt
            ds_type = ds.get('type', 'unknown')
            ds_name = ds.get('name', 'Unknown Source')
            
            # Define all supported big data types
            cloud_warehouses = ['snowflake', 'bigquery', 'redshift', 'synapse', 'databricks']
            cloud_storage = ['s3', 'azure_blob_storage', 'gcs']
            file_formats = ['csv', 'excel', 'json', 'parquet', 'orc', 'delta_lake', 'iceberg', 'hudi']
            saas_apis = ['salesforce', 'hubspot', 'stripe', 'jira', 'servicenow', 'rest_api', 'graphql_api']
            
            # Update hint if it's a known big data type
            if ds_type in cloud_warehouses + cloud_storage + file_formats + saas_apis:
                source_type_hint = ds_type
                
                # Set default dialect based on type
                if ds_type in cloud_warehouses:
                    if ds_type == 'synapse':
                        source_type_hint = 'synapse'  # SQL Server-like
                    elif ds_type == 'databricks':
                        source_type_hint = 'databricks'  # Spark SQL
                    # snowflake, bigquery, redshift already have specific dialects
                elif ds_type in cloud_storage + file_formats:
                    # For data lakes and file formats, default to DuckDB or Trino
                    source_type_hint = 'duckdb'  # Can be overridden to 'trino' if needed
                elif ds_type in saas_apis:
                    # SaaS/APIs use their own query languages (SOQL, JQL, REST, GraphQL)
                    source_type_hint = ds_type

            policies = ds.get('governance_policies') or {}
            if policies:
                governance_context.append(f"Source '{ds_name}': {json.dumps(policies)}")

        # 2. Determine operation type and query payload structure based on source type
        # Get primary source type for template customization
        primary_ds_type = data_sources[0].get('type', 'unknown') if data_sources else 'unknown'
        
        # Determine operation type and query payload structure
        if primary_ds_type in ['salesforce', 'hubspot', 'stripe', 'jira', 'servicenow', 'rest_api', 'graphql_api']:
            # SaaS/API sources use API call structure
            operation_type = "api_call"
            query_payload_template = {
                "language": "api",
                "dialect": primary_ds_type,
                "method": "GET",  # or POST, PUT, DELETE
                "endpoint": "<<API_ENDPOINT>>",
                "headers": {},
                "params": {},
                "body": {}
            }
            if primary_ds_type == 'salesforce':
                query_payload_template["language"] = "soql"
                query_payload_template["statement"] = "<<SOQL_QUERY>>"
            elif primary_ds_type == 'jira':
                query_payload_template["language"] = "jql"
                query_payload_template["jql"] = "<<JQL_QUERY>>"
            elif primary_ds_type == 'graphql_api':
                query_payload_template["language"] = "graphql"
                query_payload_template["query"] = "<<GRAPHQL_QUERY>>"
                query_payload_template["variables"] = {}
        elif primary_ds_type in ['csv', 'excel', 'json', 'parquet', 'orc', 'delta_lake', 'iceberg', 'hudi']:
            # File format sources
            operation_type = "file_query"
            query_payload_template = {
                "language": "sql",
                "dialect": "duckdb",  # or 'trino'
                "statement": "<<SQL_QUERY_WITH_FILE_FUNCTIONS>>",
                "file_path": "<<FILE_PATH>>",
                "file_format": primary_ds_type
            }
        else:
            # SQL-based sources (cloud warehouses, data lakes)
            operation_type = "source_query"
            query_payload_template = {
                "language": "sql",
                "dialect": source_type_hint if source_type_hint != 'unknown' else 'snowflake',
                "statement": "<<SQL_QUERY>>"
            }
        
        # 2. Define Strict Output Template
        response_template = {
            "request_id": payload.get("request_id"),
            "status": "success",
            "intent_type": "query",  # or 'transform', 'api_call'
            "execution_plan": {
                "strategy": "pushdown" if primary_ds_type not in ['csv', 'excel', 'json'] else "internal_compute",
                "type": "sql_query" if operation_type != "api_call" else "api_query",
                "operations": [
                    {
                        "step": 1,
                        "type": operation_type,  # 'source_query', 'file_query', 'api_call'
                        "operation_type": "read",
                        "data_source_id": primary_ds_id,
                        "query": "<<QUERY_OR_API_CALL>>",
                        "query_payload": query_payload_template,
                        "governance_applied": {
                            "rls_rules": [],
                            "masking_rules": []
                        }
                    }
                ]
            },
            "ai_metadata": {
                "confidence_score": 0.0,
                "reasoning_steps": []
            },
            "visualization_config": [
                "Bar Chart: Total sales aggregated by region",
                "Line Graph: Time-series trend over partitioned dates"
            ]
        }

        # 3. Build the Detailed System Prompt
        # Note: We pass the full data_sources object (serialized) so the LLM sees the schema structure
        system_prompt = get_big_data_agent_prompt(
            user_prompt=payload.get('user_prompt'),
            data_sources=data_sources,
            source_type_hint=source_type_hint,
            governance_context=governance_context,
            response_template=response_template,
            feedback=feedback
        )

        # 5. LLM Execution
        completion = client.chat.completions.create(
            model=config.MODEL_NAME, # ‚úÖ Use config.MODEL_NAME
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Request ID: {payload.get('request_id')}"}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        # 6. Parse & Hydrate
        lean_response = clean_and_parse_json(completion.choices[0].message.content)
        
        # Telemetry
        generation_time_ms = int((time.time() - start_time) * 1000)
        
        # Determine if this is a write operation (no visualizations)
        operations = lean_response.get("execution_plan", {}).get("operations", [])
        is_write = any(op.get("operation_type") == "write" for op in operations)
        
        # Ensure metadata exists
        if "ai_metadata" not in lean_response:
            lean_response["ai_metadata"] = {}
            
        lean_response["ai_metadata"]["generation_time_ms"] = generation_time_ms
        lean_response["ai_metadata"]["model"] = config.MODEL_NAME
        
        # Validate and set visualizations
        if "visualization_config" in lean_response:
            lean_response["visualization"] = validate_visualizations(lean_response["visualization_config"], is_write)
        else:
            lean_response["visualization"] = [] if is_write else []

        return lean_response

    except Exception as e:
        logger.error(f"Big Data Agent Failed: {e}", exc_info=True)
        return {"error": f"Big Data Agent Failed: {str(e)}"}
        
# ==============================================================================
# 9. ML AGENT (Machine Learning Specialist)
# ==============================================================================
def ml_agent(payload: Dict[str, Any], feedback: str = None) -> Dict[str, Any]:
    """
    Step 3/4: Generates a specialized ML Execution Plan.
    Orchestrates Feature Engineering, Pre-processing, Model Training, and Evaluation.
    """
    # ‚úÖ Initialize Client & Config at Runtime
    client = get_groq_client()
    config = get_config()

    start_time = time.time()
    logger.info(f"üß† [ML Agent] Building ML Pipeline... Feedback: {bool(feedback)}")

    try:
        # 1. Context Extraction
        user_prompt = payload.get('user_prompt')
        data_sources = payload.get('data_sources', [])
        user_context = payload.get('user_context', {})
        ml_params = payload.get('execution_context', {}).get('ml_params', {})

        # 2. Define the Perfect ML Response Template
        # This structure allows for features, labels, and infrastructure strategies.
        response_template = {
            "request_id": payload.get("request_id"),
            "status": "success",
            "intent_type": "ml_orchestration",
            "execution_plan": {
                "strategy": "sequential_dag", # Options: pushdown, sequential_dag, distributed_training
                "type": "ml_workflow",
                "operations": [
                    {
                        "step": 1,
                        "operation_type": "feature_extraction",
                        "description": "Extract features and labels using SQL",
                        "query": "SELECT ...",
                        "features": [], # List of independent variables
                        "labels": [],   # List of target variables
                        "output_artifact": "training_dataset"
                    },
                    {
                        "step": 2,
                        "operation_type": "pre_processing",
                        "compute_engine": "python_kernel",
                        "description": "Data cleaning, imputation, and train/test split",
                        "logic": {
                            "imputation": "mean", # mean, median, mode
                            "scaling": "standard", # standard, min_max
                            "split_ratio": 0.8     # 80/20 split
                        },
                        "dependencies": ["step_1"]
                    },
                    {
                        "step": 3,
                        "operation_type": "model_execution",
                        "description": "Train model and evaluate performance",
                        "parameters": {
                            "task": "regression", # regression, classification, forecasting
                            "algorithm": "auto",
                            "metrics": ["rmse", "r2"] 
                        },
                        "dependencies": ["step_2"]
                    }
                ]
            },
            "ai_metadata": {
                "confidence_score": 0.0,
                "reasoning_steps": [],
                "model_task": ""
            }
        }

        # 3. Build the Architectural System Prompt
        system_prompt = get_ml_agent_prompt(
            user_prompt=user_prompt,
            data_sources=data_sources,
            ml_params=ml_params,
            user_context=user_context,
            response_template=response_template,
            feedback=feedback
        )

        # 5. LLM Execution
        completion = client.chat.completions.create(
            model=config.MODEL_NAME,
            messages=[{"role": "system", "content": system_prompt}],
            temperature=0.1,
            response_format={"type": "json_object"}
        )

        # 6. Parse & Finalize Telemetry
        lean_response = json.loads(completion.choices[0].message.content)
        generation_time_ms = int((time.time() - start_time) * 1000)
        
        if "ai_metadata" not in lean_response:
            lean_response["ai_metadata"] = {}
        
        lean_response["ai_metadata"]["generation_time_ms"] = generation_time_ms
        lean_response["ai_metadata"]["model_used"] = config.MODEL_NAME

        return lean_response

    except Exception as e:
        logger.error(f"ML Agent Error: {str(e)}", exc_info=True)
        return {"error": f"ML Planning Failed: {str(e)}"}